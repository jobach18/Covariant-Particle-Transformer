{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'awkward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mawkward\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mak\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'awkward'"
     ]
    }
   ],
   "source": [
    "import awkward as ak\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'awkward' has no attribute 'from_buffers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     container \u001b[38;5;241m=\u001b[39m {key: f[key][()] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Reconstruct the Awkward Array from buffers\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mak\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_buffers\u001b[49m(form, length, container)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(ak\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'awkward' has no attribute 'from_buffers'"
     ]
    }
   ],
   "source": [
    "# load the data from the skimmed file\n",
    "\n",
    "file='skimmed_ttbarDM.h5'\n",
    "\n",
    "with h5py.File(file, 'r') as f:\n",
    "    # Load the form and length metadata\n",
    "    form = f.attrs['form']\n",
    "    length = f.attrs['length']\n",
    "    \n",
    "    # Load the buffers (numpy arrays) from the HDF5 file\n",
    "    container = {key: f[key][()] for key in f.keys()}\n",
    "    \n",
    "    # Reconstruct the Awkward Array from buffers\n",
    "    df = ak.from_buffers(form, length, container)\n",
    "print(ak.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/bachjoer/anaconda3/envs/cpt/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2933726/1427698032.py\", line 24, in <module>\n",
      "    device = torch.device('cpu')\n",
      "/tmp/ipykernel_2933726/1427698032.py:24: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device = torch.device('cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'figure.facecolor': 'white',\n",
       " 'axes.labelcolor': '.15',\n",
       " 'xtick.direction': 'out',\n",
       " 'ytick.direction': 'out',\n",
       " 'xtick.color': '.15',\n",
       " 'ytick.color': '.15',\n",
       " 'axes.axisbelow': True,\n",
       " 'grid.linestyle': '-',\n",
       " 'text.color': '.15',\n",
       " 'font.family': ['sans-serif'],\n",
       " 'font.sans-serif': ['Arial',\n",
       "  'DejaVu Sans',\n",
       "  'Liberation Sans',\n",
       "  'Bitstream Vera Sans',\n",
       "  'sans-serif'],\n",
       " 'lines.solid_capstyle': 'round',\n",
       " 'patch.edgecolor': 'w',\n",
       " 'patch.force_edgecolor': True,\n",
       " 'image.cmap': 'rocket',\n",
       " 'xtick.top': False,\n",
       " 'ytick.right': False,\n",
       " 'axes.grid': False,\n",
       " 'axes.facecolor': 'white',\n",
       " 'axes.edgecolor': '.15',\n",
       " 'grid.color': '.8',\n",
       " 'axes.spines.left': True,\n",
       " 'axes.spines.bottom': True,\n",
       " 'axes.spines.right': True,\n",
       " 'axes.spines.top': True,\n",
       " 'xtick.bottom': False,\n",
       " 'ytick.left': False}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from pathlib import Path as Path\n",
    "import cpt_columnar\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "from cpt_columnar import ColumnarCovariantTopFormer\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "USE_GPU = True\n",
    "dtype = torch.float32 # we will be using float\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('using device:', device)\n",
    "\n",
    "from seaborn import heatmap\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "sns.axes_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Lamb optimizer with lr=0.0001\n",
      "Initializing a new model from scratch\n",
      "Model has 9.9M parameters\n"
     ]
    }
   ],
   "source": [
    "max_num_output = 2 # assert that there are at most ${max_num_output} tops\n",
    "base_dir = './trained'\n",
    "arch = 'ColumnarCovariantTopFormer'\n",
    "model_params = {\n",
    "    'in_dim': 9,\n",
    "    'out_dim': 4,\n",
    "    'max_num_output': max_num_output,\n",
    "    'hidden_dim': 256,\n",
    "    'num_convs': (6, 6),\n",
    "    'heads': 4,\n",
    "    'mass': 173,\n",
    "    'match_scale_factor': torch.FloatTensor([0, 1, 1, 0]), # used for matching dR = (dy, dphi)    \n",
    "    'p_norm': 2, # used in matching and loss\n",
    "    'beta': 0.8, # loss weight for predicting number of tops\n",
    "    'dropout': 0.,\n",
    "    'schedule_lr': False,\n",
    "    'use_gpu': USE_GPU,\n",
    "    'uniform_attention': False,\n",
    "}\n",
    "tag = 'TEST' # CHANGE THIS\n",
    "output_dir = f\"{arch}_{model_params['num_convs']}_{model_params['hidden_dim']}_{tag}\"\n",
    "output_dir = os.path.join(base_dir, output_dir)\n",
    "model_params['output_dir'] = output_dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "model = eval(arch)(**model_params).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8567,  1.3374, -0.1243,  ..., -0.0280, -0.5058,  0.8143],\n",
      "        [ 0.8772, -0.0914, -0.8506,  ..., -0.2863,  0.3045,  0.1952],\n",
      "        [-0.4699, -0.1087,  0.0434,  ..., -0.9168,  0.1374,  0.0621],\n",
      "        ...,\n",
      "        [-1.2165, -0.7947,  0.4149,  ..., -0.3132, -0.8783, -0.5963],\n",
      "        [ 0.3307, -2.1478,  1.2014,  ..., -0.3847,  1.0297, -0.4359],\n",
      "        [-0.1710, -0.0950,  1.9840,  ..., -0.6622,  1.1208, -0.5500]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class RandomFloatDataset(Dataset):\n",
    "    def __init__(self, num_samples, num_features=9):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_features = num_features\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a random float tensor of size (num_features,)\n",
    "        return torch.randn(self.num_features)\n",
    "\n",
    "# Number of samples in the dataset\n",
    "num_samples = 1000\n",
    "\n",
    "# Create the dataset\n",
    "dataset = RandomFloatDataset(num_samples)\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 256\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Example of iterating over the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch)  # Each batch is a tensor of size (batch_size, num_features)\n",
    "\n",
    "    break  # Remove this break to see more batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 512])\n",
      "torch.Size([1, 256, 512])\n",
      "tensor([[-0.9627, -0.5683, -1.2048,  ...,  0.9909,  2.0373, -1.2303],\n",
      "        [-0.0220, -0.7441,  1.0866,  ...,  1.5993, -1.5469,  1.0243],\n",
      "        [ 2.0365, -0.0915, -0.7048,  ...,  0.6779,  0.0210,  0.6760],\n",
      "        ...,\n",
      "        [-0.0292,  0.2960, -0.8754,  ...,  1.2320, -0.3449,  0.8475],\n",
      "        [-0.4934,  0.2437, -1.2067,  ...,  0.7095, -0.2214,  0.1511],\n",
      "        [-0.3732, -1.1236,  1.0205,  ...,  0.5910,  0.9714,  0.1169]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[-0.3431, -0.1989,  0.6359,  ...,  1.1756, -0.2253,  0.0421],\n",
      "        [-0.3431, -0.1989,  0.6359,  ...,  1.1756, -0.2253,  0.0421],\n",
      "        [-0.3977, -0.6976, -0.7709,  ..., -0.5505,  0.9222, -0.0180],\n",
      "        ...,\n",
      "        [-0.0973, -0.2422, -0.0712,  ..., -0.0424,  0.1970,  0.4172],\n",
      "        [-1.3989,  0.2218,  1.2338,  ..., -0.6529,  0.6559,  0.2337],\n",
      "        [-1.3989,  0.2218,  1.2338,  ..., -0.6529,  0.6559,  0.2337]],\n",
      "       grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 256 but got size 512 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/phd/ml-workspace/Covariant-Particle-Transformer/cpt_columnar.py:134\u001b[0m, in \u001b[0;36mColumnarCovariantTopFormer.forward\u001b[0;34m(self, input, inference, force_correct_num_pred)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_source)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_out)\n\u001b[0;32m--> 134\u001b[0m x_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpre_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_out\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# update x_out, p_out == None because pre_decoder is non-geometric\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# retrieve invariant attention\u001b[39;00m\n\u001b[1;32m    136\u001b[0m alpha_init \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_decoder\u001b[38;5;241m.\u001b[39mget_alpha() \u001b[38;5;66;03m# (|E|, L=1, H)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/cpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/phd/ml-workspace/Covariant-Particle-Transformer/transformer_columnar.py:101\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, x_source, x_out, p_source, p_out)\u001b[0m\n\u001b[1;32m     99\u001b[0m x_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_embedding(x_out)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_blocks:\n\u001b[0;32m--> 101\u001b[0m \tx_out, p_out \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \t\u001b[38;5;66;03m# record intermediate predictions\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_out, p_out\n",
      "File \u001b[0;32m~/anaconda3/envs/cpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/phd/ml-workspace/Covariant-Particle-Transformer/transformer_columnar.py:72\u001b[0m, in \u001b[0;36mTransformerDecoderBlock.forward\u001b[0;34m(self, x_source, p_source, x_out, p_out)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_source, p_source, x_out, p_out):\n\u001b[1;32m     71\u001b[0m \tx_out, p_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn(x_out, p_out)\n\u001b[0;32m---> 72\u001b[0m \tx_out, p_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \tx_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeedforward(x_out)\n\u001b[1;32m     74\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m x_out, p_out\n",
      "File \u001b[0;32m~/anaconda3/envs/cpt/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/workspace/phd/ml-workspace/Covariant-Particle-Transformer/transformer_columnar.py:174\u001b[0m, in \u001b[0;36mResidualCrossAttentionBlock.forward\u001b[0;34m(self, x_source, p_source, x_out, p_out)\u001b[0m\n\u001b[1;32m    172\u001b[0m x_normed_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_conv_source(x_source)\n\u001b[1;32m    173\u001b[0m x_normed_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_conv_out(x_out)\n\u001b[0;32m--> 174\u001b[0m in_feat \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_normed_source\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_normed_out\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m m_x_out, p_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(in_feat) \u001b[38;5;66;03m# not using edge attr\u001b[39;00m\n\u001b[1;32m    176\u001b[0m x_out \u001b[38;5;241m=\u001b[39m x_out \u001b[38;5;241m+\u001b[39m m_x_out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 256 but got size 512 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    model(batch) \n",
    "    \n",
    "    break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpt",
   "language": "python",
   "name": "cpt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
